{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchainhub gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Relevant Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # This loads the .env file at the application start\n",
    "password = os.getenv('passwd')\n",
    "api_key = os.getenv('api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic model used for embeddings, can be improved by using a more complex model\n",
    "model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = GPT4AllEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_id  = '802f868877384e9798b731802ffa4827:ZXVyb3BlLXdlc3QzLmdjcC5jbG91ZC5lcy5pbyQ0NzYyZTQ2YzQ5NDg0ODY5YTAzZDMxYzg5NjY2MjY3YyQ1ZjQ3NWI2NTQxOTI0NmZiODcxNDc3NjZlMTI4YWE2YQ=='\n",
    "elastic_vector_search = ElasticsearchStore(\n",
    "    es_cloud_id=cloud_id,\n",
    "    index_name=\"embeddings_index\",\n",
    "    embedding=embedding,\n",
    "    es_user=\"group13\",\n",
    "    es_password=password,\n",
    "    es_api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Inwieweit wird in der Organisation Informationssicherheit gemanagt?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the most basic retrieval method for now, to be experimented with\n",
    "retriever = elastic_vector_search.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 20})\n",
    "\n",
    "retrieved_docs = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_texts = [result.page_content for result in retrieved_docs]  # adjust the key according to your result structure\n",
    "\n",
    "# Concatenate these texts into a single string to provide as context\n",
    "context = \" \".join(document_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Prompt for Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain import hub\n",
    "\n",
    "# Says max 3 sentences, can change accoriding to the requirement\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir('c:\\\\Users\\\\rafay\\\\OneDrive\\\\Desktop\\\\Masters\\\\DS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "# model downloaded using gpt4all ui, path pointing to model\n",
    "# model_path = \"/Users/I748655/Library/Application Support/nomic.ai/GPT4All/Meta-Llama-3-8B-Instruct.Q4_0.gguf\"\n",
    "model_path = \"/Users/omeriqbal/Downloads/Meta-Llama-3-8B-Instruct.Q4_0.gguf\"\n",
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "llm = GPT4All(model=model_path, callbacks=callbacks, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(results):\n",
    "    return \"\\n\\n\".join(result.page_content for result in results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | build_context, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in rag_chain.stream(question):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The risk analysis is conducted by the management and auditors. The methodology used includes document review or interviews with users, administrators, and other relevant stakeholders. The risk analysis is conducted by the management and auditors. The methodology used includes document review or interviews with users, administrators, and other relevant stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Testing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_data import test_questions\n",
    "generated_responses2 = []\n",
    "for i, question in enumerate(test_questions):\n",
    "    response = \"\"\n",
    "    print(\"\\nQuestion no: \", i+1)\n",
    "    for chunk in rag_chain.stream(question):\n",
    "        response+=chunk\n",
    "        #I added a print just in case that the chunks from the previous question remain inside the buffer for the next question\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    \n",
    "    generated_responses2.append(response)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save('generated_responses2.npy',generated_responses2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm.generate([\"What is the capital of France?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_data import test_questions,test_responses\n",
    "\n",
    "print(len(test_questions))\n",
    "print(len(test_responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_responses = []\n",
    "for i, question in enumerate(test_questions):\n",
    "    response = \"\"\n",
    "    print(\"\\nQuestion no: \", i+1)\n",
    "    for chunk in rag_chain.stream(question):\n",
    "        response+=chunk\n",
    "        #I added a print just in case that the chunks from the previous question remain inside the buffer for the next question\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    \n",
    "    generated_responses.append(response)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save('DataScienceGroup13/src/generated_responses.npy',generated_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_responses=np.load(\"generated_responses2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s=[]\n",
    "\n",
    "for test_response, gen_response in zip(test_responses,generated_responses):\n",
    "    # Tokenize the sentence\n",
    "    response_words = word_tokenize(test_response)\n",
    "    golden_words = word_tokenize(gen_response)\n",
    "    \n",
    "    # Filter out punctuation\n",
    "    response_words = [word for word in response_words if word.isalnum()]\n",
    "    golden_words = [word for word in golden_words if word.isalnum()]\n",
    "\n",
    "    # Convert arrays to sets\n",
    "    response_set = set(response_words)\n",
    "    gen_set = set(golden_words)\n",
    "\n",
    "    # Find the intersection of the two sets\n",
    "    intersection = response_set.intersection(gen_set)\n",
    "\n",
    "    # Get the number of shared elements\n",
    "    num_shared_elements = len(intersection)\n",
    "    pred_length = len(response_words)\n",
    "    gold_length = len(golden_words)\n",
    "    \n",
    "    precision= num_shared_elements/pred_length\n",
    "    recall= num_shared_elements/gold_length\n",
    "    f1=2*precision*recall/(precision+recall)\n",
    "    \n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1s.append(f1)\n",
    "\n",
    "print(\"Precision:\", np.mean(precisions))\n",
    "print(\"Recall:\", np.mean(recalls))\n",
    "print(\"F1 score:\", np.mean(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_responses = np.array(test_responses)\n",
    "test_unicode = test_responses.astype('<U1049')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bleu wants matching 4-grams which aren't found in German texts usually\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "\n",
    "# tokenized_references = [word_tokenize(resp) for resp in test_responses[0]]\n",
    "# tokenized_predictions = [word_tokenize(gen) for gen in generated_responses[0]]\n",
    "\n",
    "tokenized_references = word_tokenize(test_responses[0]) \n",
    "tokenized_predictions = word_tokenize(generated_responses[0])\n",
    "\n",
    "\n",
    "\n",
    "print(tokenized_predictions)\n",
    "print(tokenized_references)\n",
    "print(len(tokenized_references))\n",
    "print(len(tokenized_predictions))\n",
    "\n",
    "# # Calculate BLEU score\n",
    "# bleu_score = corpus_bleu(list_of_references=[[tokenized_references]],hypotheses= [tokenized_predictions])\n",
    "bleu_score = corpus_bleu(list_of_references=[[['zur', 'Informationssicherheit','in']]],hypotheses= [['zur', 'Informationssicherheit','in']])\n",
    "print(f\"BLEU score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PIP INSTALL EVALUATE\n",
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "results = bertscore.compute(predictions=generated_responses, references=test_unicode, lang=\"de\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BERT precision:\" ,np.mean(results['precision']))\n",
    "print(\"BERT Recall:\" ,np.mean(results['recall']))\n",
    "print(\"BERT F1:\", np.mean(results['f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleurt = load(\"bleurt\", module_type=\"metric\")\n",
    "bleurt_results = bleurt.compute(predictions=generated_responses, references=test_unicode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BLEURT SCORE: \", np.mean(bleurt_results['scores']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor= load(\"meteor\")\n",
    "\n",
    "meteor_results = meteor.compute(predictions=generated_responses, references=test_unicode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meteor_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu= load(\"bleu\")\n",
    "\n",
    "bleu_results = bleu.compute(predictions=generated_responses, references=test_unicode)\n",
    "print(bleu_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs_list = []\n",
    "for i, question in enumerate(test_questions):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    \n",
    "    content = []\n",
    "    for doc in retrieved_docs:\n",
    "        content.append(doc.page_content)\n",
    "    \n",
    "    retrieved_docs_list.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"retrieved_docs_list.npy\",retrieved_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents_list = np.load(\"DataScienceGroup13/src/retrieved_docs_list.npy\")\n",
    "retrieved_docs_list = []\n",
    "for doc in retrieved_documents_list:\n",
    "    retrieved_docs_list.append(doc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_resp=llmJudge.generate(\n",
    "prompts = [\"GEBEN SIE NUR PUNKTE!!! Geben Sie dem folgenden Filmtitel aus dem Horror-Genre eine Punktzahl zwischen 1 und 5:Flüstern der Ewigkeit\"],\\\n",
    "        max_tokens=5,temp=0,top_p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
    "GEBEN SIE NUR PUNKTE!!! Sie müssen mit der Bewertung beginnen\n",
    "Sie erhalten eine Antwort, die von einem RAG-Modell (Retrieval-Augmented Generation) für eine bestimmte Abfrage generiert wird. Ihre Aufgabe besteht darin, die Antwort anhand einer Metrik zu bewerten.\n",
    "Bitte stellen Sie sicher, dass Sie diese Anweisungen sorgfältig lesen und verstehen. \n",
    "Bitte lassen Sie dieses Dokument während der Durchsicht geöffnet und schlagen Sie bei Bedarf darin nach.\n",
    "\n",
    "Evaluationskriterien:\n",
    "\n",
    "{Kriterien}\n",
    "\n",
    "Bewertungsschritte:\n",
    "\n",
    "{Schritte}\n",
    "\n",
    "Beispiel:\n",
    "\n",
    "Abfrage:\n",
    "\n",
    "{Abfrage}\n",
    "\n",
    "Abgerufene Dokumente:\n",
    "\n",
    "{retrieved_documents}\n",
    "\n",
    "Grundwahrheit:\n",
    "\n",
    "{ground_truth}\n",
    "\n",
    "Generierte Antwort:\n",
    "\n",
    "{Antwort}\n",
    "\n",
    "Bewertungsformular (NUR Ergebnisse):\n",
    "\n",
    "- {metric_name}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_PRECISION_CRITERIA = \"\"\"\n",
    "Kontextgenauigkeit (1–5) – misst das Signal-Rausch-Verhältnis des abgerufenen Kontexts. \\\n",
    "Die abgerufenen Dokumente sollten einen hohen Anteil relevanter Informationen enthalten, die zur Beantwortung der Anfrage erforderlich sind, sowie einen minimalen Anteil irrelevanter Informationen.\n",
    "\"\"\"\n",
    "\n",
    "CONTEXT_PRECISION_STEPS = \"\"\"\n",
    "1. Lesen Sie die Anfrage und die abgerufenen Dokumente sorgfältig durch.\n",
    "2. Identifizieren Sie, wie viele der abgerufenen Informationen für die Beantwortung der Anfrage relevant sind.\n",
    "3. Weisen Sie basierend auf dem Anteil relevanter Informationen eine Kontextpräzisionsbewertung von 1 bis 5 zu.\n",
    "\"\"\"\n",
    "CONTEXT_RECALL_CRITERIA = \"\"\"\n",
    "Kontextrückruf (1-5) – misst, ob alle relevanten Informationen, die zur Beantwortung der Anfrage erforderlich sind, abgerufen wurden. \\\n",
    "Die abgerufenen Dokumente sollten alle notwendigen Informationen enthalten, die zur umfassenden Beantwortung der Anfrage erforderlich sind.\n",
    "\"\"\"\n",
    "\n",
    "CONTEXT_RECALL_STEPS = \"\"\"\n",
    "1. Lesen Sie die Anfrage, die abgerufenen Dokumente und die Ground-Truth-Informationen sorgfältig durch.\n",
    "2. Stellen Sie fest, ob die abgerufenen Dokumente alle relevanten Informationen enthalten, die zur Beantwortung der Anfrage erforderlich sind.\n",
    "3. Weisen Sie basierend auf der Vollständigkeit der abgerufenen Informationen einen Kontextrückruf-Score von 1 bis 5 zu.\n",
    "\"\"\"\n",
    "FAITHFULNESS_CRITERIA = \"\"\"\n",
    "Treue (1-5) – misst die sachliche Richtigkeit der generierten Antwort. \\\n",
    "Die Antwort sollte nur Aussagen enthalten, die von den abgerufenen Dokumenten unterstützt werden.\n",
    "\"\"\"\n",
    "\n",
    "FAITHFULNESS_STEPS = \"\"\"\n",
    "1. Lesen Sie die Anfrage, die abgerufenen Dokumente und die generierte Antwort sorgfältig durch.\n",
    "2. Identifizieren Sie die Aussagen in der Antwort und vergleichen Sie sie jeweils mit den abgerufenen Dokumenten auf sachliche Richtigkeit.\n",
    "3. Weisen Sie basierend auf dem Anteil richtiger Aussagen einen Treuewert von 1 bis 5 zu.\n",
    "\"\"\"\n",
    "\n",
    "ANSWER_RELEVANCY_CRITERIA = \"\"\"\n",
    "Antwortrelevanz (1–5) – misst, wie relevant die generierte Antwort für die Anfrage ist. \\\n",
    "Die Antwort sollte alle Teile der Anfrage umfassend und genau beantworten.\n",
    "\"\"\"\n",
    "\n",
    "ANSWER_RELEVANCY_STEPS = \"\"\"\n",
    "1. Lesen Sie die Abfrage und die generierte Antwort sorgfältig durch.\n",
    "2. Bestimmen Sie, wie gut die Antwort auf die Anfrage eingeht, einschließlich aller Aspekte der Frage.\n",
    "3. Weisen Sie eine Antwortrelevanzbewertung von 1 bis 5 zu, basierend auf der Relevanz und Vollständigkeit der Antwort.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_score(\n",
    "    criteria: str, steps: str, query: str, retrieved_documents: list, ground_truth: str, response: str, metric_name: str\n",
    "):\n",
    "    retrieved_documents_str = \"\\n\".join(retrieved_documents)\n",
    "    prompt = EVALUATION_PROMPT_TEMPLATE.format(\n",
    "        Kriterien=criteria,\n",
    "        Schritte=steps,\n",
    "        metric_name=metric_name,\n",
    "        Abfrage=query,\n",
    "        retrieved_documents=retrieved_documents_str,\n",
    "        ground_truth=ground_truth,\n",
    "        Antwort=response,\n",
    "    )\n",
    "    response=llmJudge.generate(prompts = [prompt],max_tokens=5,temp=0.1,top_p=1)\n",
    "    \n",
    "    return response.generations[0][0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_first_german_number(text):\n",
    "    # Regular expression to match numbers with comma as decimal separator\n",
    "    match = re.search(r'\\d+(,\\d+)?', text)\n",
    "    if match:\n",
    "        # If a match is found, replace comma with dot and convert to float\n",
    "        number_str = match.group().replace(',', '.')\n",
    "        return float(number_str) if '.' in number_str else int(number_str)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "evaluation_metrics = {\n",
    "    \"Context Precision\": (CONTEXT_PRECISION_CRITERIA, CONTEXT_PRECISION_STEPS),\n",
    "    \"Context Recall\": (CONTEXT_RECALL_CRITERIA, CONTEXT_RECALL_STEPS),\n",
    "    \"Faithfulness\": (FAITHFULNESS_CRITERIA, FAITHFULNESS_STEPS),\n",
    "    \"Answer Relevancy\": (ANSWER_RELEVANCY_CRITERIA, ANSWER_RELEVANCY_STEPS),\n",
    "}\n",
    "\n",
    "queries = copy.deepcopy(test_questions)\n",
    "retrieved_documents_list = copy.deepcopy(retrieved_docs_list)\n",
    "responses = copy.deepcopy(generated_responses)\n",
    "ground_truths = copy.deepcopy(test_responses)\n",
    "\n",
    "data = {\"Evaluation Type\": [], \"Query Type\": [], \"Score\": []}\n",
    "\n",
    "for eval_type, (criteria, steps) in evaluation_metrics.items():\n",
    "    for i, (query, retrieved_documents, response, ground_truth) in enumerate(zip(queries, retrieved_documents_list, responses, ground_truths)):\n",
    "        data[\"Evaluation Type\"].append(eval_type)\n",
    "        data[\"Query Type\"].append(f\"Query {i + 1}\")\n",
    "        result = get_rag_score(criteria, steps, query, retrieved_documents, ground_truth, response, eval_type)\n",
    "        print(result)\n",
    "        score_num = get_first_german_number(result)\n",
    "        print(score_num)\n",
    "        data[\"Score\"].append(score_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"G_Eval.py\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_precision = data['Score'][:41]\n",
    "context_recall = data['Score'][41:82]\n",
    "faithfulness = data['Score'][82:123]\n",
    "answer_relevancy = data['Score'][123:164]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_precision = [i for i in context_precision if i != 0]\n",
    "context_recall = [i for i in context_recall if i != 0]\n",
    "faithfulness = [i for i in faithfulness if i != 0]\n",
    "answer_relevancy = [i for i in answer_relevancy if i != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Context Precision Score\", np.mean(context_precision))\n",
    "print(\"Context Recall Score\",np.mean(context_recall))\n",
    "print(\"Faithfulness Score\",np.mean(faithfulness))\n",
    "print(\"Answer Relevancy Score\",np.mean(answer_relevancy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
