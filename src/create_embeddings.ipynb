{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain\n",
    "# %pip install langchain-community\n",
    "# %pip install pypdf\n",
    "# %pip install unstructured\n",
    "# %pip install openpyxl\n",
    "# %pip install --upgrade --quiet  gpt4all > /dev/null\n",
    "# %pip install --upgrade --quiet langchain-elasticsearch langchain-openai tiktoken langchain\n",
    "# %pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain.document_loaders import PyPDFLoader, CSVLoader\n",
    "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
    "from langchain.docstore.document import Document\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_excel_to_csv(directory):\n",
    "    # Get a list of all Excel files in the directory\n",
    "    excel_files = [file for file in os.listdir(directory) if file.endswith(('.xls', '.xlsx'))]\n",
    "    for excel_file in excel_files:\n",
    "        # Define the path to the Excel file\n",
    "        excel_file_path = os.path.join(directory, excel_file)\n",
    "        \n",
    "        # Read all sheets from the Excel file\n",
    "        excel_data = pd.read_excel(excel_file_path, sheet_name=None)\n",
    "\n",
    "        # Process each sheet separately\n",
    "        for sheet_name, df in excel_data.items():\n",
    "            # Define the output CSV file path, including the sheet name\n",
    "            output_csv_file = os.path.join(directory, f\"{os.path.splitext(excel_file)[0]}_{sheet_name}.csv\")\n",
    "\n",
    "            # Drop unnamed columns\n",
    "            df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "            \n",
    "            # Drop rows where all values are NaN or columns that are entirely NaN\n",
    "            df.dropna(how='all', inplace=True)\n",
    "            df.dropna(axis=1, how='all', inplace=True)\n",
    "            \n",
    "            # Save the DataFrame to a CSV file\n",
    "            df.to_csv(output_csv_file, index=False, encoding='utf-8')\n",
    "            \n",
    "            print(f\"Converted {excel_file} - Sheet: {sheet_name} to {output_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/home/ethel/Documents/SOSE 2024/DATA SCIENCE/project/DataScienceGroup13/src/KnowledgeBase\"\n",
    "convert_excel_to_csv(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDocs = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        print(\"Processing\", filename)\n",
    "        pdfLoader = PyPDFLoader(os.path.join(directory, filename))\n",
    "\n",
    "        if \"pdfs\" not in allDocs:\n",
    "            allDocs[\"pdfs\"] = []\n",
    "\n",
    "        allDocs[\"pdfs\"].extend(pdfLoader.load())\n",
    "\n",
    "    # elif filename.endswith(\".xlsx\"):\n",
    "    #     print(\"Processing\", filename)\n",
    "    #     excelLoader = UnstructuredExcelLoader(os.path.join(directory, filename))\n",
    "\n",
    "\n",
    "    #     if \"xlsx\" not in allDocs:\n",
    "    #         allDocs[\"xlsx\"] = []\n",
    "\n",
    "    #     allDocs[\"xlsx\"].extend(excelLoader.load())\n",
    "\n",
    "    # elif filename.endswith(\".csv\"):\n",
    "    #     print(\"Processing\", filename)\n",
    "    #     csvloader = CSVLoader(os.path.join(directory, filename))\n",
    "\n",
    "    #     if \"csv\" not in allDocs:\n",
    "    #         allDocs[\"csv\"] = []\n",
    "\n",
    "    #     allDocs[\"csv\"].extend(csvloader.load())\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert csv file to Langchain doc\n",
    "def process_csv(file_path):\n",
    "    docs = []\n",
    "    with open(file_path, newline=\"\", encoding='utf-8-sig') as csvfile:\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        columns = csv_reader.fieldnames  # Get the column names dynamically\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            to_metadata = {col: row[col] for col in columns if col in row}\n",
    "            values_to_embed = {k: row[k] for k in columns if k in row}\n",
    "            to_embed = \"\\n\".join(f\"{k.strip()}: {v.strip()}\" for k, v in values_to_embed.items())\n",
    "            newDoc = Document(page_content=to_embed, metadata=to_metadata)\n",
    "            docs.append(newDoc)\n",
    "    return docs\n",
    "allDocs['csv'] = []\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        allDocs['csv'].extend(process_csv(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loaded\", len(allDocs[\"pdfs\"]), \"pdf pages\")\n",
    "# print(\"Loaded\", len(allDocs[\"xlsx\"]), \"excel sheets\")\n",
    "print(\"Loaded\", len(allDocs[\"csv\"]), \"CSV rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"First excel page content: \\n\", allDocs[\"xlsx\"][0].page_content[0:1000])\n",
    "print(\"CSV row: \\n\", allDocs[\"csv\"][400].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First pdf page content: \\n\", allDocs[\"pdfs\"][0].page_content[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"First excel page metadata:\", allDocs[\"csv\"][0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First pdf page metadata:\", allDocs[\"pdfs\"][78].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# r_splitter_excel = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=150,\n",
    "#     chunk_overlap=0,\n",
    "#     separators=[\"\\r\\n\", \"\\n\", \"\\t\", \",\", \" \"]\n",
    "# )\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=0,\n",
    "    length_function=len\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits={}\n",
    "\n",
    "for key in allDocs:\n",
    "    splits[key] = r_splitter.split_documents(allDocs[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in allDocs:\n",
    "    print(\"Number of splits for\", key, \":\", len(splits[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 5 PDF splits: \", splits[\"pdfs\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"First 5 Excel splits\", splits[\"xlsx\"][:5])\n",
    "# print(\"First 5 CSV splits\", splits[\"csv\"][:5])\n",
    "splits[\"csv\"][0].page_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = GPT4AllEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"i like dogs\"\n",
    "sentence2 = \"i like canines\"\n",
    "sentence3 = \"the weather is ugly outside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "embedding3 = embedding.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # This loads the .env file at the application start\n",
    "password = os.getenv('passwd')\n",
    "api_key = os.getenv('api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_id  = '802f868877384e9798b731802ffa4827:ZXVyb3BlLXdlc3QzLmdjcC5jbG91ZC5lcy5pbyQ0NzYyZTQ2YzQ5NDg0ODY5YTAzZDMxYzg5NjY2MjY3YyQ1ZjQ3NWI2NTQxOTI0NmZiODcxNDc3NjZlMTI4YWE2YQ=='\n",
    "elastic_vector_search = ElasticsearchStore(\n",
    "    es_cloud_id=cloud_id,\n",
    "    index_name=\"embeddings_index\",\n",
    "    embedding=embedding,\n",
    "    es_user=\"group13\",\n",
    "    es_password=password,\n",
    "    es_api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elastic_vector_search.delete(\"embeddings_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_vector_search.add_documents(splits[\"pdfs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_vector_search.add_documents(splits[\"csv\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is responsible for conducting the risk analysis and what methodology is used\"\n",
    "\n",
    "results = elastic_vector_search.similarity_search(question, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].page_content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
